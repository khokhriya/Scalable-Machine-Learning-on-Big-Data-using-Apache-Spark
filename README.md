# Scalable-Machine-Learning-on-Big-Data-using-Apache-Spark
## Introduction
This is an introduction to Apache Spark. You'll learn how Apache Spark internally works and how to use it for data processing. RDD, the low level API is introduced in conjunction with parallel programming / functional programming. Then, different types of data storage solutions are contrasted. Finally, Apache Spark SQL and the optimizer Tungsten and Catalyst are explained.
* Understanding key concepts of distributed computing for big data - Understanding key concepts on Apache Spark and how it achieves parallelization  -Understanding how an Apache Spark program looks like
* Describe what BigData means
* Summarize advantages and disanvantages of different storage solutions
* Explain how parallelization is achieved in ApacheSpark
* Write parallel ApacheSpark code using functional programming and the RDD API
* Summarize how SparkSQL works
* Compare SQL and DataFrames against RDDs
* Explain the advantages of Tungsten and Catalyst optimizers

## Scaling Math for Statistics on Apache Spark
Applying basic statistical calculations using the Apache Spark RDD API in order to experience how parallelization in Apache Spark works
* Averages
* Standard deviation
* Skewness
* Kurtosis
* Covariance, Covariance matrices, correlation
* Plotting with ApacheSpark and python's matplotlib
* Dimensionality reduction
* PCA5

## Introduction to Apache SparkML
* Summarize properties of machine learning pipelines
* Express machine learning pipelines programatically using Apache SparkML
* Examine the importance of data preparation
* Explain clustering and the k-means algorithm
* Analyze any BigData data set using k-means and Apache SparkML

## Supervised and Unsupervised learning with SparkML
* Create a Apache SparkML Pipelines for Linear Regression
* Create a Apache SparkML Pipelines for Logistic Regression
* Analyze a given data set using Apache SparkML
